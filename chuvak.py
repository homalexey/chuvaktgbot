import re
import logging
import urllib.parse
from telegram import Update
from telegram.ext import Application, MessageHandler, filters, ContextTypes
import wikipediaapi
import requests
from bs4 import BeautifulSoup
import asyncio
import cloudscraper
import os
from openai import OpenAI

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
TELEGRAM_TOKEN = os.environ['TELEGRAM_TOKEN']
GROQ_API_KEY = os.environ['GROQ_API_KEY']

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Groq (—Å–æ–≤–º–µ—Å—Ç–∏–º —Å OpenAI API)
groq_client = OpenAI(
    api_key=GROQ_API_KEY,
    base_url="https://api.groq.com/openai/v1"
)

logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

wiki_wiki = wikipediaapi.Wikipedia(
    language='ru',
    extract_format=wikipediaapi.ExtractFormat.WIKI,
    user_agent='ChuvakSharitBot/1.0 (https://t.me/ChuvakSharitBot)'
)

# === –§–£–ù–ö–¶–ò–ò –ü–û–ò–°–ö–ê ===

def get_wikipedia(term: str) -> str:
    try:
        candidates = [term.strip(), term.strip().title(), term.strip().capitalize()]
        candidates = list(dict.fromkeys(candidates))
        for candidate in candidates:
            page = wiki_wiki.page(candidate)
            if page.exists():
                if "–º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å" in page.summary or "–∑–Ω–∞—á–µ–Ω–∏—è" in page.summary[:100]:
                    continue
                summary = page.summary[:900]
                return f"üî∏ *–í–∏–∫–∏–ø–µ–¥–∏—è*: {summary}‚Ä¶"
        return "üî∏ *–í–∏–∫–∏–ø–µ–¥–∏—è*: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    except Exception as e:
        logger.error(f"Wikipedia error: {e}")
        return "üî∏ *–í–∏–∫–∏–ø–µ–¥–∏—è*: –æ—à–∏–±–∫–∞"

def get_wiktionary(term: str) -> str:
    try:
        url = f"https://ru.wiktionary.org/wiki/{urllib.parse.quote(term)}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=6)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            for tag in soup(['script', 'style', 'sup', '.mw-editsection', '.reference']):
                tag.decompose()
            meaning_heading = soup.find('span', {'id': '–ó–Ω–∞—á–µ–Ω–∏–µ'})
            if meaning_heading:
                parent = meaning_heading.find_parent()
                if parent:
                    ol = parent.find_next(['ol', 'ul'])
                    if ol:
                        text = ol.get_text(' ', strip=True)
                        text = re.sub(r'\[\d+\]|\(.*?\)|\d+\.', '', text)
                        text = re.sub(r'\s+', ' ', text).strip()
                        if len(text) > 20:
                            return f"üîπ *–í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—å*: {text[:800]}‚Ä¶"
            content = soup.find('div', {'class': 'mw-parser-output'})
            if content:
                p = content.find('p')
                if p:
                    text = p.get_text(' ', strip=True)
                    if len(text) > 30:
                        return f"üîπ *–í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—å*: {text[:600]}‚Ä¶"
            return "üîπ *–í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—å*: –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
        else:
            return "üîπ *–í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—å*: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"
    except Exception as e:
        logger.warning(f"Wiktionary error: {e}")
        return "üîπ *–í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—å*: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞"

def get_lurk(term: str) -> str:
    try:
        term_norm = term.strip().title().replace(' ', '_')
        encoded_term = urllib.parse.quote(term_norm)
        scraper = cloudscraper.create_scraper(
            browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
        )
        url = f"https://lurkmore.media/{encoded_term}"
        response = scraper.get(url, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', '.infobox', '.mw-editsection']):
                tag.decompose()
            content_div = soup.find('div', id='mw-content-text')
            if content_div:
                first_block = content_div.find(['p', 'div'])
                if first_block:
                    text = first_block.get_text(' ', strip=True)
                    text = re.sub(r'\[.*?\]|\(.*?\)|\b(?:–ø—Ä–∞–≤–∏—Ç—å|—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å)\b', '', text)
                    text = re.sub(r'\s+', ' ', text).strip()
                    if len(text) > 30 and "Loading" not in text and "Cloudflare" not in text:
                        return f"üî∂ *Lurk.media*: {text[:900]}‚Ä¶"
            return "üî∂ *Lurk.media*: —Å—Ç–∞—Ç—å—è –µ—Å—Ç—å, –Ω–æ –Ω–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è"
        else:
            return "üî∂ *Lurk.media*: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    except Exception as e:
        logger.warning(f"Lurk.media error: {e}")
        return "üî∂ *Lurk.media*: –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏"

def get_gramota(term: str) -> str:
    try:
        url = f"https://gramota.ru/poisk?query={urllib.parse.quote(term)}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=6)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            result = soup.find('div', class_=re.compile(r'(card|result|search|entry)', re.IGNORECASE))
            if not result:
                result = soup.find(['p', 'div'], string=re.compile(r'.{10,}'))
            if result:
                text = result.get_text(' ', strip=True)
                if len(text) > 30 and not any(t in text for t in ["–ü–æ–¥–ø–∏—Å–∫–∞", "–†–µ–∫–ª–∞–º–∞", "–°–ª–æ–≤–æ –¥–Ω—è", "¬©", "–ì—Ä–∞–º–æ—Ç–∞.—Ä—É"]):
                    return f"üìò *–ì—Ä–∞–º–æ—Ç–∞.—Ä—É*: {text[:800]}‚Ä¶"
            return "üìò *–ì—Ä–∞–º–æ—Ç–∞.—Ä—É*: –Ω–µ –Ω–∞—à—ë–ª –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"
        else:
            return "üìò *–ì—Ä–∞–º–æ—Ç–∞.—Ä—É*: –æ—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞"
    except Exception as e:
        logger.warning(f"Gramota error: {e}")
        return "üìò *–ì—Ä–∞–º–æ—Ç–∞.—Ä—É*: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞"

def get_academic(term: str) -> str:
    try:
        url = f"https://dic.academic.ru/dic.nsf/ru/{urllib.parse.quote(term)}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=6)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            for tag in soup(['script', 'style', 'nav', 'footer', '.nav', '.footer']):
                tag.decompose()
            content = soup.find('div', class_=re.compile(r'(card|content|main)', re.I))
            if not content:
                content = soup
            definition = content.find(['dd', 'p', 'div'], string=re.compile(r'.{10,}'))
            if definition:
                text = definition.get_text(' ', strip=True)
                if len(text) > 25 and not any(t in text for t in ["–°–º. —Ç–∞–∫–∂–µ", "¬©", "Academic.ru", "–ù–∞—É—á–Ω–æ-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π", "–≠–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏—á–µ—Å–∫–∏–π"]):
                    return f"üìö *Academic.ru*: {text[:800]}‚Ä¶"
            return "üìö *Academic.ru*: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
        else:
            return "üìö *Academic.ru*: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    except Exception as e:
        logger.warning(f"Academic.ru error: {e}")
        return "üìö *Academic.ru*: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞"

def get_urban(term: str) -> str:
    try:
        url = f"https://api.urbandictionary.com/v0/define?term={urllib.parse.quote(term)}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=6)
        if response.status_code == 200:
            data = response.json()
            if data.get('list'):
                definition = data['list'][0].get('definition', '').replace('\n', ' ').strip()
                example = data['list'][0].get('example', '').replace('\n', ' ').strip()
                if definition:
                    text = definition
                    if example and len(example) > 10:
                        text += f" –ü—Ä–∏–º–µ—Ä: {example}"
                    return f"üá∫üá∏ *Urban Dict*: {text[:800]}‚Ä¶"
            return "üá∫üá∏ *Urban Dict*: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
        else:
            return "üá∫üá∏ *Urban Dict*: –æ—à–∏–±–∫–∞ API"
    except Exception as e:
        logger.warning(f"Urban error: {e}")
        return "üá∫üá∏ *Urban Dict*: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞"

# === –û–ë–†–ê–ë–û–¢–ß–ò–ö –°–û–û–ë–©–ï–ù–ò–ô ===

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    # –ó–∞—â–∏—Ç–∞: –µ—Å–ª–∏ –Ω–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–ª–∏ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞ ‚Äî –≤—ã—Ö–æ–¥–∏–º
    if not update.message or not update.message.text:
        return

    text = update.message.text.strip()
    chat_type = update.message.chat.type

    # –í –ª–∏—á–∫–µ ‚Äî –ª—é–±–æ–π –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å
    if chat_type == 'private':
        clean_text = re.sub(r'[^\w\s]', '', text).strip()
        if clean_text and len(clean_text.split()) <= 5:
            await process_query(update, clean_text)
        return

    # –í –≥—Ä—É–ø–ø–µ ‚Äî —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ò–õ–ò —Ñ—Ä–∞–∑–∞ —Å "–ß—É–≤–∞–∫"
    if chat_type != 'private':
        if update.message.text.startswith(f"@{context.bot.username}"):
            text = update.message.text[len(f"@{context.bot.username}"):].strip()
            if text:
                await process_query(update, text)
            return

        pattern = re.compile(r'^(—á—É–≤—Å—Ç–≤?–∞–∫\s*,?\s+)(—á—Ç–æ\s+—Ç–∞–∫–æ–µ|–∫—Ç–æ\s+—Ç–∞–∫–æ–π)\s+(.+)', re.IGNORECASE)
        match = pattern.search(text)
        if match:
            term = match.group(3).strip(' ?.')
            if term:
                await process_query(update, term)
        return

# === –°–ò–ù–¢–ï–ó –û–¢–í–ï–¢–ê –ß–ï–†–ï–ó GROQ (LLAMA 3.1) ===

async def process_query(update: Update, term: str):
    await update.message.chat.send_action(action="typing")

    loop = asyncio.get_event_loop()
    tasks = [
        loop.run_in_executor(None, get_wikipedia, term),
        loop.run_in_executor(None, get_wiktionary, term),
        loop.run_in_executor(None, get_lurk, term),
        loop.run_in_executor(None, get_gramota, term),
        loop.run_in_executor(None, get_academic, term),
        loop.run_in_executor(None, get_urban, term),
    ]

    results = await asyncio.gather(*tasks)

    clean_facts = []
    for res in results:
        if ":" in res:
            content = res.split(":", 1)[1].strip()
            if content and content not in ["‚Ä¶", "–Ω–µ –Ω–∞–π–¥–µ–Ω–æ", "–æ—à–∏–±–∫–∞", "–Ω–µ –Ω–∞–π–¥–µ–Ω–∞", "–∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ", "—Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"]:
                clean_facts.append(res)

    if not clean_facts:
        final_text = "–ß—É–≤–∞–∫ —Å–µ–≥–æ–¥–Ω—è –Ω–µ –≤ —Ñ–æ—Ä–º–µ, –Ω–æ –≤–æ—Ç —á—Ç–æ –Ω–∞—Ä—ã–ª:\n" + "\n".join(results)
    else:
        context = "\n".join(clean_facts)
        prompt = f'''
–¢—ã ‚Äî ¬´–ß—É–≤–∞–∫¬ª, —É–º–Ω—ã–π, –∏—Ä–æ–Ω–∏—á–Ω—ã–π, –Ω–æ —Ç–æ—á–Ω—ã–π –¥—Ä—É–≥. –ù–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∏–∂–µ –¥–∞–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: ¬´–ß—Ç–æ —Ç–∞–∫–æ–µ {term}?¬ª –∏–ª–∏ ¬´–ö—Ç–æ —Ç–∞–∫–æ–π {term}?¬ª.

–ü—Ä–∞–≤–∏–ª–∞:
- 2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è,
- –Ω–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–º —Ä—É—Å—Å–∫–æ–º,
- –±–µ–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–Ω–µ –ø–∏—à–∏ ¬´–í–∏–∫–∏–ø–µ–¥–∏—è –≥–æ–≤–æ—Ä–∏—Ç...¬ª),
- –º–æ–∂–Ω–æ —Å –ª—ë–≥–∫–∏–º —Å–ª–µ–Ω–≥–æ–º (—Ç–∏–ø–æ ¬´—à–∞—Ä–∏—Ç¬ª, ¬´–ª–æ–ª¬ª, ¬´–º–µ–º¬ª, ¬´—Ç—Ä–µ—à¬ª),
- –∫–∞–∫ –±—É–¥—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—à—å –¥—Ä—É–≥—É –≤ —á–∞—Ç–µ.

–î–∞–Ω–Ω—ã–µ:
{context}
'''

        try:
            print(f"[GROQ] –ó–∞–ø—Ä–∞—à–∏–≤–∞—é –æ—Ç–≤–µ—Ç –¥–ª—è: {term}")
            llm_response = await loop.run_in_executor(None, lambda: groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=250
            ))
            final_text = llm_response.choices[0].message.content.strip()
            print(f"[GROQ] –û—Ç–≤–µ—Ç: {final_text[:100]}...")
        except Exception as e:
            logger.error(f"Groq error: {e}")
            print(f"[GROQ ERROR] {e}")
            final_text = "–ß—É–≤–∞–∫ —à–∞—Ä–∏—Ç, –Ω–æ —Å–µ–≥–æ–¥–Ω—è –ª–µ–Ω—å –æ–±—ä—è—Å–Ω—è—Ç—å. –í–æ—Ç —á—Ç–æ –Ω–∞—à—ë–ª:\n" + "\n".join(clean_facts)

    response = f'üîç *{term.capitalize()}*\n\n{final_text}\n\n‚Äî –û–±—Ä–∞—â–∞–π—Å—è, —á—É–≤–∞–∫'
    if len(response) > 4000:
        response = response[:3990] + '‚Ä¶'

    await update.message.reply_text(response, parse_mode="Markdown", disable_web_page_preview=True)

# === –ó–ê–ü–£–°–ö ===

import os

def main():
    app = Application.builder().token(TELEGRAM_TOKEN).build()
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    PORT = int(os.environ.get('PORT', 8000))
    WEBHOOK_URL = os.environ.get('WEBHOOK_URL')

    if not WEBHOOK_URL:
        raise ValueError("WEBHOOK_URL –Ω–µ –∑–∞–¥–∞–Ω. –£–∫–∞–∂–∏—Ç–µ –µ–≥–æ –≤ Environment Variables –Ω–∞ Render.")

    app.run_webhook(
        listen="0.0.0.0",
        port=PORT,
        url_path=TELEGRAM_TOKEN,
        webhook_url=f"{WEBHOOK_URL}/{TELEGRAM_TOKEN}"
    )

if __name__ == '__main__':
    main()
