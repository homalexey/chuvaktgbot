import re
import logging
import urllib.parse
from telegram import Update
from telegram.ext import Application, MessageHandler, filters, ContextTypes
import wikipediaapi
import requests
from bs4 import BeautifulSoup
import asyncio
import cloudscraper
import os
import concurrent.futures
from openai import OpenAI
executor = concurrent.futures.ThreadPoolExecutor(max_workers=6)

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
TELEGRAM_TOKEN = os.environ['TELEGRAM_TOKEN']
GROQ_API_KEY = os.environ['GROQ_API_KEY']

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Groq (—Å–æ–≤–º–µ—Å—Ç–∏–º —Å OpenAI API)
groq_client = OpenAI(
    api_key=GROQ_API_KEY,
    base_url="https://api.groq.com/openai/v1"
)

logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

wiki_wiki = wikipediaapi.Wikipedia(
    language='ru',
    extract_format=wikipediaapi.ExtractFormat.WIKI,
    user_agent='ChuvakSharitBot/1.0 (https://t.me/ChuvakSharitBot)'
)

# === –§–£–ù–ö–¶–ò–ò –ü–û–ò–°–ö–ê ===

def get_wikipedia(term: str) -> str:
    try:
        candidates = [term.strip(), term.strip().title(), term.strip().capitalize()]
        candidates = list(dict.fromkeys(candidates))
        for candidate in candidates:
            page = wiki_wiki.page(candidate)
            if page.exists():
                if "–º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å" in page.summary or "–∑–Ω–∞—á–µ–Ω–∏—è" in page.summary[:100]:
                    continue
                summary = page.summary[:900]
                return f"üî∏ *–í–∏–∫–∏–ø–µ–¥–∏—è*: {summary}‚Ä¶"
        return "üî∏ *–í–∏–∫–∏–ø–µ–¥–∏—è*: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    except Exception as e:
        logger.error(f"Wikipedia error: {e}")
        return "üî∏ *–í–∏–∫–∏–ø–µ–¥–∏—è*: –æ—à–∏–±–∫–∞"

def get_wiktionary(term: str) -> str:
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        variants = [term, term.lower(), term.capitalize(), term.upper()]
        for cand in variants:
            url = f"https://ru.wiktionary.org/wiki/{urllib.parse.quote(cand)}"
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, "html.parser")
                content = soup.find("div", class_="mw-parser-output")
                if content:
                    p = content.find("p")
                    if p and len(p.get_text(strip=True)) > 30:
                        return f"üîπ *–í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—å*: {p.get_text(' ', strip=True)[:600]}‚Ä¶"
            elif resp.status_code == 404 and cand.isupper():
                # –ü–æ–ø—Ä–æ–±—É–µ–º –∞–Ω–≥–ª. –≤–µ—Ä—Å–∏—é
                url_en = f"https://en.wiktionary.org/wiki/{urllib.parse.quote(cand)}"
                resp = requests.get(url_en, headers=headers, timeout=10)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.text, "html.parser")
                    p = soup.find("p")
                    if p and len(p.get_text(strip=True)) > 30:
                        return f"üîπ *Wiktionary (EN)*: {p.get_text(' ', strip=True)[:600]}‚Ä¶"
        return "üîπ *–í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—å*: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    except Exception as e:
        logger.warning(f"Wiktionary error: {e}")
        return "üîπ *–í–∏–∫–∏—Å–ª–æ–≤–∞—Ä—å*: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞"


def get_lurk(term: str) -> str:
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç–∞—Ç—å—é —Å Lurkmore.media (—á–µ—Ä–µ–∑ cloudscraper)."""
    try:
        term_norm = term.strip().replace(" ", "_").capitalize()
        url = f"https://lurkmore.media/{urllib.parse.quote(term_norm)}"

        scraper = cloudscraper.create_scraper(
            browser={"browser": "chrome", "platform": "windows", "mobile": False}
        )
        scraper.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0 Safari/537.36",
            "Referer": "https://lurkmore.media/"
        })

        response = scraper.get(url, timeout=12)

        if response.status_code != 200:
            logger.info(f"Lurk.media {term} HTTP {response.status_code}")
            return "üî∂ *Lurk.media*: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"

        soup = BeautifulSoup(response.text, "html.parser")
        # —É–¥–∞–ª–∏—Ç—å –º—É—Å–æ—Ä
        for tag in soup(["script", "style", "nav", "header", "footer", "table", "sup"]):
            tag.decompose()

        content_div = soup.find("div", id="mw-content-text")
        if not content_div:
            return "üî∂ *Lurk.media*: —Å—Ç–∞—Ç—å—è –µ—Å—Ç—å, –Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"

        # –∏—â–µ–º –ø–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
        p = content_div.find(["p", "div"], string=re.compile(r".{30,}"))
        if not p:
            return "üî∂ *Lurk.media*: —Å—Ç–∞—Ç—å—è –µ—Å—Ç—å, –Ω–æ –Ω–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è"

        text = p.get_text(" ", strip=True)
        text = re.sub(r"\[.*?\]|\(.*?\)", "", text)
        text = re.sub(r"\s+", " ", text).strip()
        if not text or len(text) < 30:
            return "üî∂ *Lurk.media*: —Å—Ç–∞—Ç—å—è –ø—É—Å—Ç–∞—è"

        return f"üî∂ *Lurk.media*: {text[:900]}‚Ä¶"
    except Exception as e:
        logger.warning(f"Lurk.media error for {term}: {e}")
        return "üî∂ *Lurk.media*: –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏"

def get_gramota(term: str) -> str:
    """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å gramota.ru."""
    try:
        url = f"https://gramota.ru/poisk?query={urllib.parse.quote(term)}"
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=12)
        if resp.status_code != 200:
            logger.info(f"Gramota.ru {term} HTTP {resp.status_code}")
            return "üìò *–ì—Ä–∞–º–æ—Ç–∞.—Ä—É*: –æ—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞"

        soup = BeautifulSoup(resp.text, "html.parser")
        # –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
        result = soup.find("div", class_=re.compile(r"(card|result|entry|content)", re.I))
        if not result:
            result = soup.find("p", string=re.compile(r".{15,}"))

        if result:
            text = result.get_text(" ", strip=True)
            # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º—É—Å–æ—Ä–∞
            if any(bad in text for bad in ["¬©", "–†–µ–∫–ª–∞–º–∞", "–ü–æ–¥–ø–∏—Å–∫–∞", "–ì—Ä–∞–º–æ—Ç–∞.—Ä—É"]):
                logger.debug(f"Gramota skipped noise for {term}")
                return "üìò *–ì—Ä–∞–º–æ—Ç–∞.—Ä—É*: –Ω–µ –Ω–∞—à—ë–ª –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"
            return f"üìò *–ì—Ä–∞–º–æ—Ç–∞.—Ä—É*: {text[:800]}‚Ä¶"

        return "üìò *–ì—Ä–∞–º–æ—Ç–∞.—Ä—É*: –Ω–µ –Ω–∞—à—ë–ª –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"
    except Exception as e:
        logger.warning(f"Gramota error for {term}: {e}")
        return "üìò *–ì—Ä–∞–º–æ—Ç–∞.—Ä—É*: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞"

def get_academic(term: str) -> str:
    """–ü–∞—Ä—Å–∏—Ç dic.academic.ru."""
    try:
        url = f"https://dic.academic.ru/dic.nsf/ru/{urllib.parse.quote(term)}"
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=12)
        if resp.status_code != 200:
            logger.info(f"Academic.ru {term} HTTP {resp.status_code}")
            return "üìö *Academic.ru*: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"

        soup = BeautifulSoup(resp.text, "html.parser")
        for tag in soup(["script", "style", "footer", "nav", "sup"]):
            tag.decompose()

        # –≤–æ–∑–º–æ–∂–Ω—ã–µ –±–ª–æ–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content = soup.find("div", class_=re.compile(r"(content|card|main|entry)", re.I)) or soup
        # –∏—â–µ–º –ø–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ/–æ–ø–∏—Å–∞–Ω–∏–µ
        definition = content.find(["dd", "p", "div"], string=re.compile(r".{20,}"))
        if not definition:
            return "üìö *Academic.ru*: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

        text = definition.get_text(" ", strip=True)
        if len(text) < 25:
            return "üìö *Academic.ru*: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ"
        if any(bad in text for bad in ["¬©", "–°–º. —Ç–∞–∫–∂–µ", "Academic.ru"]):
            return "üìö *Academic.ru*: –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"

        return f"üìö *Academic.ru*: {text[:900]}‚Ä¶"
    except Exception as e:
        logger.warning(f"Academic error for {term}: {e}")
        return "üìö *Academic.ru*: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞"

def get_urban(term: str) -> str:
    """–ò—â–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ Urban Dictionary (–∞–Ω–≥–ª.)."""
    try:
        url = f"https://api.urbandictionary.com/v0/define?term={urllib.parse.quote(term)}"
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=12)
        if response.status_code != 200:
            logger.warning(f"UrbanDict HTTP {response.status_code} for {term}")
            return "üá∫üá∏ *Urban Dict*: –æ—à–∏–±–∫–∞ API"

        data = response.json()
        if not data.get("list"):
            return "üá∫üá∏ *Urban Dict*: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

        # –±–µ—Ä—ë–º —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        best = max(data["list"], key=lambda d: len(d.get("definition", "")))
        definition = best.get("definition", "").replace("\n", " ").strip()
        example = best.get("example", "").replace("\n", " ").strip()
        text = definition
        if example and len(example) > 10:
            text += f" –ü—Ä–∏–º–µ—Ä: {example}"
        return f"üá∫üá∏ *Urban Dict*: {text[:900]}‚Ä¶"
    except Exception as e:
        logger.warning(f"UrbanDict error for {term}: {e}")
        return "üá∫üá∏ *Urban Dict*: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞"

# === –û–ë–†–ê–ë–û–¢–ß–ò–ö –°–û–û–ë–©–ï–ù–ò–ô ===

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    # –ó–∞—â–∏—Ç–∞: –µ—Å–ª–∏ –Ω–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–ª–∏ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞ ‚Äî –≤—ã—Ö–æ–¥–∏–º
    if not update.message or not update.message.text:
        return

    text = update.message.text.strip()
    chat_type = update.message.chat.type

    # –í –ª–∏—á–∫–µ ‚Äî –ª—é–±–æ–π –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å
    if chat_type == 'private':
        clean_text = re.sub(r'[^\w\s]', '', text).strip()
        if clean_text and len(clean_text.split()) <= 5:
            await process_query(update, clean_text)
        return

    # –í –≥—Ä—É–ø–ø–µ ‚Äî —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ò–õ–ò —Ñ—Ä–∞–∑–∞ —Å "–ß—É–≤–∞–∫"
    if chat_type != 'private':
        if update.message.text.startswith(f"@{context.bot.username}"):
            text = update.message.text[len(f"@{context.bot.username}"):].strip()
            if text:
                await process_query(update, text)
            return

        pattern = re.compile(r'^(—á—É–≤–∞–∫\s*,?\s+)(—á—Ç–æ\s+—Ç–∞–∫–æ–µ|–∫—Ç–æ\s+—Ç–∞–∫–æ–π)\s+(.+)', re.IGNORECASE)
        match = pattern.search(text)
        if match:
            term = match.group(3).strip(' ?.')
            if term:
                await process_query(update, term)
        return

# === –°–ò–ù–¢–ï–ó –û–¢–í–ï–¢–ê –ß–ï–†–ï–ó GROQ (LLAMA 3.1) ===

async def process_query(update: Update, term: str):
    await update.message.chat.send_action(action="typing")

    loop = asyncio.get_event_loop()
    tasks = [
        loop.run_in_executor(executor, get_wikipedia, term),
        loop.run_in_executor(executor, get_wiktionary, term),
        loop.run_in_executor(executor, get_lurk, term),
        loop.run_in_executor(executor, get_gramota, term),
        loop.run_in_executor(executor, get_academic, term),
        loop.run_in_executor(executor, get_urban, term),
    ]

    # –≤–∞–∂–Ω–æ: –Ω–µ –ø–∞–¥–∞–µ–º –ø—Ä–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è—Ö –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–¥—Ä–æ–±–Ω–æ ‚Äî —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç –¥–µ–±–∞–≥—É –≤ Render logs
    for i, res in enumerate(results):
        source = ["wikipedia","wiktionary","lurk","gramota","academic","urban"][i]
        if isinstance(res, Exception):
            logger.warning(f"[{term}] source={source} -> EXCEPTION: {res}")
        else:
            logger.info(f"[{term}] source={source} -> {len(str(res))} chars, preview: {str(res)[:120]!r}")

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ‚Äî –µ—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç exception, –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ —Ç–µ–∫—Å—Ç-–æ—à–∏–±–∫—É
    normalized = []
    for res in results:
        if isinstance(res, Exception):
            normalized.append("–æ—à–∏–±–∫–∞")
        else:
            normalized.append(res)

    clean_facts = []
    for res in normalized:
        if isinstance(res, str) and ":" in res:
            content = res.split(":", 1)[1].strip()
            if content and content.lower() not in ["‚Ä¶", "–Ω–µ –Ω–∞–π–¥–µ–Ω–æ", "–æ—à–∏–±–∫–∞", "–Ω–µ –Ω–∞–π–¥–µ–Ω–∞", "–∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ", "—Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"]:
                clean_facts.append(res)

    if not clean_facts:
        # –ø–æ–∫–∞–∂–µ–º –∏ —á—Ç–æ –∏–º–µ–Ω–Ω–æ –≤–µ—Ä–Ω—É–ª–æ—Å—å (–ø–æ–ª–µ–∑–Ω–æ –≤ –ª–æ–≥–∞—Ö)
        final_text = "–ß—É–≤–∞–∫ —Å–µ–≥–æ–¥–Ω—è –Ω–µ –≤ —Ñ–æ—Ä–º–µ, –Ω–æ –≤–æ—Ç —á—Ç–æ –Ω–∞—Ä—ã–ª:\n" + "\n".join(normalized)
    else:
        context = "\n".join(clean_facts)
        prompt = f'''
–¢—ã ‚Äî ¬´–ß—É–≤–∞–∫¬ª, —É–º–Ω—ã–π, –∏—Ä–æ–Ω–∏—á–Ω—ã–π, –Ω–æ —Ç–æ—á–Ω—ã–π –¥—Ä—É–≥. –ù–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∏–∂–µ –¥–∞–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: ¬´–ß—Ç–æ —Ç–∞–∫–æ–µ {term}?¬ª –∏–ª–∏ ¬´–ö—Ç–æ —Ç–∞–∫–æ–π {term}?¬ª.

–ü—Ä–∞–≤–∏–ª–∞:
- 2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è,
- –Ω–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–º —Ä—É—Å—Å–∫–æ–º,
- –±–µ–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–Ω–µ –ø–∏—à–∏ ¬´–í–∏–∫–∏–ø–µ–¥–∏—è –≥–æ–≤–æ—Ä–∏—Ç...¬ª),
- –º–æ–∂–Ω–æ —Å –ª—ë–≥–∫–∏–º —Å–ª–µ–Ω–≥–æ–º (—Ç–∏–ø–æ ¬´—à–∞—Ä–∏—Ç¬ª, ¬´–ª–æ–ª¬ª, ¬´–º–µ–º¬ª, ¬´—Ç—Ä–µ—à¬ª, ¬´—Ä–æ—Ñ–ª¬ª, ¬´—Ö–∞–π–ø¬ª, ¬´–∫—Ä–∏–Ω–∂¬ª),
- –∫–∞–∫ –±—É–¥—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—à—å –¥—Ä—É–≥—É –≤ —á–∞—Ç–µ.

–î–∞–Ω–Ω—ã–µ:
{context}
'''
        try:
            logger.info(f"[GROQ] –ó–∞–ø—Ä–∞—à–∏–≤–∞—é –æ—Ç–≤–µ—Ç –¥–ª—è: {term}")
            # Groq call (–∫–∞–∫ —É —Ç–µ–±—è –±—ã–ª–æ)
            llm_response = await loop.run_in_executor(None, lambda: groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=250
            ))
            final_text = llm_response.choices[0].message.content.strip()
            logger.info(f"[GROQ] –û—Ç–≤–µ—Ç: {final_text[:100]}...")
        except Exception as e:
            logger.error(f"Groq error: {e}")
            final_text = "–ß—É–≤–∞–∫ —à–∞—Ä–∏—Ç, –Ω–æ —Å–µ–≥–æ–¥–Ω—è –ª–µ–Ω—å –æ–±—ä—è—Å–Ω—è—Ç—å. –í–æ—Ç —á—Ç–æ –Ω–∞—à—ë–ª:\n" + "\n".join(clean_facts)

    response = f'üîç *{term.capitalize()}*\n\n{final_text}\n\n‚Äî –û–±—Ä–∞—â–∞–π—Å—è, —á—É–≤–∞–∫'
    if len(response) > 4000:
        response = response[:3990] + '‚Ä¶'

    await update.message.reply_text(response, parse_mode="Markdown", disable_web_page_preview=True)

# === –ó–ê–ü–£–°–ö ===

import os

def main():
    app = Application.builder().token(TELEGRAM_TOKEN).build()
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    PORT = int(os.environ.get('PORT', 8000))
    WEBHOOK_URL = os.environ.get('WEBHOOK_URL')

    if not WEBHOOK_URL:
        raise ValueError("WEBHOOK_URL –Ω–µ –∑–∞–¥–∞–Ω. –£–∫–∞–∂–∏—Ç–µ –µ–≥–æ –≤ Environment Variables –Ω–∞ Render.")

    app.run_webhook(
        listen="0.0.0.0",
        port=PORT,
        url_path=TELEGRAM_TOKEN,
        webhook_url=f"{WEBHOOK_URL}/{TELEGRAM_TOKEN}"
    )

if __name__ == '__main__':
    main()

